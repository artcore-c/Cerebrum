# Cerebrumâ„¢ - Distributed Interactive AI Code Assistant

### Token-streaming code generation optimized for Raspberry Pi CM4 + cloud architecture

> Real-time AI code completion and refactoring running on a Clockwork Pi uConsole **Raspberry Pi CM4**, powered by VPS inference and intelligent context management.

<p align="center">
  <img src="docs/diagrams/images/Cerebrum_CLI-(SSH).jpg" alt="Cerebrum_CLI(SSH)" width="725"/>
</p>

## What Makes Cerebrum Different

**Streaming-First Design**
> Token-by-token Server-Sent Events (SSE) for real-time code generation feedback

**Intelligent Context Management**
> Smart chunking reduces 8KB+ prompts by up to 62% while preserving refactoring instructions

**Edge Orchestration**
> Raspberry Pi CM4 handles routing, chunking, and request coordination with <100ms overhead

**Production-Grade Resilience**
> Circuit breakers, connection pooling, load shedding, and request correlation IDs throughout

**Language-Aware Model Routing**
> Automatic selection between Qwen-7B (Python/JS) and CodeLLaMA-7B (Rust/C/C++) per request

**Lightweight CLI Interface**
> Streaming REPL with multiline support, command history, and live token display

## Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raspberry Pi CM4 (Orchestrator)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  FastAPI Server (Port 7000)                       â”‚  â”‚
â”‚  â”‚  â€¢ Instruction extraction & prompt assembly       â”‚  â”‚
â”‚  â”‚  â€¢ Smart chunking (1000 char blocks, 150 overlap) â”‚  â”‚
â”‚  â”‚  â€¢ Deduplication (hash-based fingerprinting)      â”‚  â”‚
â”‚  â”‚  â€¢ Load shedding (max 2 concurrent requests)      â”‚  â”‚
â”‚  â”‚  â€¢ Request tracking (UUID correlation)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ HTTP/Tailscale (Streaming SSE)
                    â”‚ Chunked prompts â†’ Token stream
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VPS Inference Backend (Port 9000)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  llama.cpp Runtime                                â”‚  â”‚
â”‚  â”‚  â€¢ Model: qwen-7b-q4.gguf / codellama-7b-q4.gguf  â”‚  â”‚
â”‚  â”‚  â€¢ Inference: ~1.6 tok/s (CPU, single-threaded)   â”‚  â”‚
â”‚  â”‚  â€¢ Connection pool: Persistent httpx client       â”‚  â”‚
â”‚  â”‚  â€¢ Circuit breaker: 10s cooldown on failures      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
Data Flow:

1. User prompt â†’ CM4 extracts instructions
2. CM4 chunks large code (if >1500 chars)
3. CM4 deduplicates repeated patterns
4. CM4 selects top 3 relevant chunks
5. CM4 assembles instruction-first prompt
6. VPS streams tokens back via SSE
7. CM4 proxies stream to client in real-time

---
## Real-World Performance and Capabilities
**Intelligent Prompt Handling**
- Instruction extraction (e.g. refactor / rewrite / TODO directives)
- Instruction-first prompt assembly for base code models
- Automatic fallback to raw prompts when transformation is not beneficial

**Smart Chunking & Deduplication**
- Chunks only when prompts exceed safe thresholds
- Deduplicates overlapping code blocks
- Uses task-aware ranking (instruction-driven, not naive similarity)
- Skips chunking entirely when reduction is insignificant

**Streaming Inference:**
- Small prompts (<100 chars): ~17s for 33 tokens (1.9 tok/s)
- Large prompts (8KB): ~182s for 129 tokens (0.7 tok/s) after 62% chunking reduction
- CM4 overhead: <100ms for chunking + routing

**Context Management:**
- Input: 8,344 chars (repeated synchronous code)
- After chunking: 3,167 chars (62% reduction)
- Result: Actual async/await refactored code (not TODO lists!)

**Resource-Aware Design:**
- Max concurrent: 2 requests (load shedding)
- Circuit breaker: 10s cooldown after VPS failures
- Request timeout: Configurable per endpoint
- Connection pooling: Persistent HTTP client (no repeated initialization)

**Interactive REPL + API**
- Bash-based interactive shell for fast iteration
- Full FastAPI surface for automation and tooling

---

## ğŸš€ Quick Start
**Prerequisites**
- Raspberry Pi CM4 (4GB RAM 0GB eMMC Lite)
- VPS with 4GB+ RAM (8GB+ for multiple large models running simultainiously)
- Python 3.11+
- Deployment Models Pre-installed (See below)

### 1. Start VPS Backend
```
# On VPS
cd ~/cerebrum-backend
./start.sh

# Verify health
curl http://localhost:9000/health
```
### 2. Start CM4 Orchestrator
```
# On Raspberry Pi
cd /opt/cerebrum-pi
./start.sh

# Verify health
curl http://localhost:7000/health
```
### 3. Launch Streaming REPL
```
cd /opt/cerebrum-pi/scripts
./cerebrum_repl.sh
```
**REPL Commands:**
```
>>> :help              Show commands
>>> :model qwen_7b     Switch model
>>> :lang python       Set language
>>> :multi             Toggle multiline mode
>>> def fibonacci(n):  Generate code!
```
---

## Deployment Model

Cerebrum is composed of **two independently deployed systems**

### CM4 Orchestrator (Raspberry Pi)

**The CM4 never runs large models. It decides *what* to send, *how much* to send,** 
**and *how* to stream results back efficiently.**
- Runs continuously on the Pi
- Handles all user interaction
- Enforces safety and performance constraints

### ğŸ“˜ Deployment Guide:  
[`cerebrum-pi/README.md`](./cerebrum-pi/README.md)

---

### VPS Inference Backend

**Runs heavy LLM inference using `llama.cpp` with strict resource controls**
- The backend supports multiple GGUF models via llama.cpp-compatible runtimes
- Models are selected dynamically at request time
- Exposes inference and streaming endpoints
- Tuned for CPU/GPU efficiency

### ğŸ“™ Deployment Guide:  
[`cerebrum-backend/README.md`](./cerebrum-backend/README.md)

---

**Note:**  
The root of this repository is **not directly executable**.  
All runtime instructions live in the component-specific READMEs above.

## ğŸ“‚ Project Structure
```
Cerebrum/                        # ğŸ© Root
â”‚
â”œâ”€â”€ cerebrum-pi/                   # ğŸ”¹ CM4 Orchestrator (Raspberry Pi)
â”‚   â”œâ”€â”€ cerebrum/
â”‚   â”‚   â”œâ”€â”€ api/                     # âœ¨ FastAPI Application (Active)
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py                  # Application entry point
â”‚   â”‚   â”‚   â”œâ”€â”€ middleware/              # Request processing
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ request_id.py        # UUID correlation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ log_context.py       # Request logging
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ load_shed.py         # Concurrency limiting
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/              # ğŸ’« API endpoints
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ inference.py         # Streaming code completion
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ _chunking_helper.py  # Smart prompt processing
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ health.py            # Health checks
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ models.py            # Model listing
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ stats.py             # System statistics
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ schemas/             # ğŸ”® API schemas / future Pydantic models
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ core/                    # ğŸª„ VPS Integration (Active)
â”‚   â”‚   â”‚   â””â”€â”€ vps_client.py            # Connection pooling, circuit breaker
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ retrieval/               # ğŸ§¬ Context Management (Active)
â”‚   â”‚   â”‚   â”œâ”€â”€ chunker.py               # Text chunking (1000 char blocks)
â”‚   â”‚   â”‚   â”œâ”€â”€ ranker.py                # Relevance ranking + deduplication
â”‚   â”‚   â”‚   â”œâ”€â”€ assembler.py             # Prompt assembly
â”‚   â”‚   â”‚   â””â”€â”€ instruction_parser.py    # Instruction extraction
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ orchestration/           # ğŸ”® Future: Multi-step task coordination
â”‚   â”‚   â”œâ”€â”€ reasoning/               # ğŸ”® Future: Symbolic / constraint-based reasoning
â”‚   â”‚   â”œâ”€â”€ tasks/                   # ğŸ”® Future: Reusable task templates
â”‚   â”‚   â””â”€â”€ utils/                   # ğŸ”® Future: Shared helper functions
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ cerebrum_repl.sh             # Interactive streaming CLI
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ cerebrum-tunnel.service      # Tailscale VPN systemd service
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                        # ğŸ“„ Runtime data
â”‚   â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ knowledge_base/
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                       # ğŸ§ª Test suites
â”‚   â”‚   â”œâ”€â”€ test_api/
â”‚   â”‚   â”œâ”€â”€ test_core/
â”‚   â”‚   â””â”€â”€ test_integration/
â”‚   â”‚
â”‚   â”œâ”€â”€ start.sh                         # Start orchestrator
â”‚   â”œâ”€â”€ stop.sh                          # Stop orchestrator
â”‚   â””â”€â”€ requirements.txt                 # Python dependencies
â”‚
â”œâ”€â”€ cerebrum-backend/              # ğŸ”¸ VPS Inference Backend
â”‚   â”œâ”€â”€ vps_server/                  # âš™ï¸ Inference Engine (Active)
â”‚   â”‚   â””â”€â”€ main.py                      # FastAPI + llama.cpp streaming
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ start.sh
â”‚   â”‚   â”œâ”€â”€ test.sh                      # Health check tests
â”‚   â”‚   â””â”€â”€ generate_api_key.sh          # API key generation
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                          # Configuration files
â”‚   â”œâ”€â”€ logs/                            # Runtime logs
â”‚   â”œâ”€â”€ cerebrum-backend.service         # Systemd service
â”‚   â””â”€â”€ requirements.txt                 # Python dependencies
â”‚
â”œâ”€â”€ deployment/                      # ğŸ”® Future: Deployment Automation
â”‚   â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ systemd/
â”‚
â”œâ”€â”€ docs/                            # ğŸ“š Documentation
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ API.md
â”‚   â”œâ”€â”€ architecture/
â”‚   â”‚   â””â”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ diagrams/
â”‚   â”‚   â””â”€â”€ images/
â”‚   â”œâ”€â”€ guides/
â”‚   â”‚   â””â”€â”€ DEVELOPMENT.md
â”‚   â””â”€â”€ optimization/
â”‚       â””â”€â”€ PERFORMANCE.md
â”‚   
â”œâ”€â”€ scripts/                        # ğŸ”§ Development Tools
â”‚   â”œâ”€â”€ sync_to_cm4.sh                  # Rsync to Raspberry Pi
â”‚   â””â”€â”€ sync_to_vps.sh                  # Rsync to VPS
â”‚
â””â”€â”€ shared/                         # ğŸ§º Shared Resources
    â”œâ”€â”€ embeddings/                     # Vector embeddings cache
    â”œâ”€â”€ knowledge_base/                 # Curated reference material
    â”‚   â”œâ”€â”€ code_snippets/              # Reusable code examples
    â”‚   â”œâ”€â”€ documentation/              # External reference materials
    â”‚   â”‚   â””â”€â”€ vendor_docs/            # Official API docs, language specs
    â”‚   â””â”€â”€ examples/                   # Sample projects
    â””â”€â”€ models/
        â”œâ”€â”€ download_scripts/           # Model acquisition utilities
        â”‚   â””â”€â”€ download_models.sh
        â””â”€â”€ lists/                      # Model manifests / allowlists
```

## â˜•ï¸ Development Workflow

1. Edit on macOS (VS Code + VS Code Insider)
2. Sync to CM4 (`rsync`)
3. Sync to VPS (`rsync`)
4. Test locally via REPL or API
5. Iterate without redeploying the full system

This workflow enables rapid iteration despite a split architecture.

## ğŸ“š Documentation

See `docs/` directory for detailed information:
- [Architecture](./docs/architecture/Architecture.md)
- [API Reference](./docs/api/API.md)
- [Optimization](./docs/optimization/PERFORMANCE.md)

## License

Cerebrumâ„¢ Â© 2025 Robert Hall. All rights reserved.

This project is licensed under the [MIT License](./LICENSE).

---

## Acknowledgments

Built with:
- [FastAPI](https://fastapi.tiangolo.com/) - High-performance async web framework
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Efficient LLM inference
- [httpx](https://www.python-httpx.org/) - Modern HTTP client with connection pooling
- [Qwen](https://github.com/QwenLM/Qwen) - Alibaba's excellent code model

Inspired by the challenge of running production AI on edge devices.

---

**Questions? Issues? PRs welcome!**